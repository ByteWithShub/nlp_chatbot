{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver \n",
    "!pip install gradio --quiet\n",
    "!pip install selenium webdriver-manager\n",
    "!pip install beautifulsoup4 requests\n",
    "!pip install -q transformers sentence-transformers\n",
    "!pip install --upgrade torch transformers sentence-transformers\n",
    "from functools import partial\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from IPython import get_ipython\n",
    "from transformers import pipeline\n",
    "from IPython.display import display, Markdown\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from sentence_transformers import SentenceTransformer, util "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03171445",
   "metadata": {},
   "source": [
    "Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_text(url, tag='p'):\n",
    "\n",
    "    #Setting custom headers to mimic a real browser and avoid basic bot blocks\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "\n",
    "        #GET request to the specified URL\n",
    "        #If the request was successful\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            #Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            #Extracting all elements with the specified tag\n",
    "            elements = soup.find_all(tag)\n",
    "\n",
    "            #Fallback: if very few <p> tags are found, we try grabbing <div> content\n",
    "            if len(elements) < 3 and tag == 'p':\n",
    "                elements = soup.find_all('div')\n",
    "\n",
    "            #returning all extracted text, separated by newlines\n",
    "            return \"\\n\\n\".join([e.get_text(strip=True) for e in elements])\n",
    "        else:\n",
    "\n",
    "            #Printing error if page request fails\n",
    "            print(f\"Error {response.status_code} on {url}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "\n",
    "        #Handling exceptions such as connection errors or timeouts\n",
    "        print(f\"Exception at {url}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the 'ww2_sources' folder if it doesn't already exist\n",
    "os.makedirs(\"ww2_sources\", exist_ok=True)\n",
    "\n",
    "\n",
    "#Loops through each source and its URL\n",
    "for name, url in sources.items():\n",
    "    print(f\"Scraping: {name}\")\n",
    "\n",
    "    #Scrapes article text from the URL\n",
    "    content = scrape_article_text(url, tag='p')\n",
    "\n",
    "    if content:\n",
    "        new_words = len(content.split())\n",
    "\n",
    "        #Creates a clean filename from the source name\n",
    "        filename = name.lower().replace(\" \", \"_\") + \".txt\"\n",
    "        file_path = f\"ww2_sources/{filename}\"\n",
    "\n",
    "\n",
    "        #If the file already exists, check the word count in the old content\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                old_content = f.read()\n",
    "            old_words = len(old_content.split())\n",
    "        else:\n",
    "            old_words = 0\n",
    "\n",
    "        #Save new content only if it has more or equal words than the old one\n",
    "        if new_words >= old_words:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            print(f\"Saved: {filename} ({new_words} words)\")\n",
    "        else:\n",
    "            print(f\"Skipped: {filename} (new={new_words} < old={old_words})\")\n",
    "    else:\n",
    "        print(f\"No content scraped for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27637698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing total word counter\n",
    "total_words = 0\n",
    "print(\"\\nFile Summary:\")\n",
    "\n",
    "#Loop through each file in the 'ww2_sources' directory\n",
    "for file in os.listdir(\"ww2_sources\"):\n",
    "    path = os.path.join(\"ww2_sources\", file)\n",
    "\n",
    "    #Open file in read mode\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        wc = len(content.split())\n",
    "        total_words += wc\n",
    "        print(f\"{file}: {wc} words\")\n",
    "\n",
    "print(f\"\\nTOTAL DATASET SIZE: {total_words} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d269b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backing up original folder\n",
    "!cp -r ww2_sources ww2_sources_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a557dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking content of source folder\n",
    "!ls ww2_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_scrape_and_save(name, url, tag='p', folder='ww2_timelines'):\n",
    "\n",
    "    #Create folder if it doesn't exist\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"Scraping: {name}\")\n",
    "\n",
    "    #Scapes textual content from the given url\n",
    "    content = scrape_article_text(url, tag)\n",
    "\n",
    "    if content:\n",
    "        #Count number of words in the newly scraped content\n",
    "        new_words = len(content.split())\n",
    "\n",
    "        #Formats, cleans and lowercases filename\n",
    "        filename = name.lower().replace(\" \", \"_\").replace(\"/\", \"_\") + \".txt\"\n",
    "        file_path = os.path.join(folder, filename)\n",
    "\n",
    "\n",
    "        #Checks if same file exists if yes, then counts word in existing\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "              old_words = len(f.read().split())\n",
    "\n",
    "        #If no file exists, treat old word count as zero\n",
    "        else:\n",
    "            old_words = 0\n",
    "\n",
    "        #Only overwrites the file if the new content has more or equal words\n",
    "        if new_words >= old_words:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            print(f\"Saved: {filename} ({new_words} words)\")\n",
    "        else:\n",
    "            print(f\"Skipped: {filename} (new={new_words} < old={old_words})\")\n",
    "    else:\n",
    "        print(f\"No content scraped for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2653ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each timeline source and safely scrape + save its content to the 'ww2_timelines' folder\n",
    "for name, url in timeline_sources.items():\n",
    "    safe_scrape_and_save(name, url, tag='p', folder='ww2_timelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5107fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking content for timeline folder\n",
    "!ls ww2_timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previewing timeline folder content\n",
    "file_path = os.path.join(\"ww2_timelines\", \"thoughtco_timeline.txt\")\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e47a11",
   "metadata": {},
   "source": [
    "Building Model\n",
    "\n",
    "Note: For this project we are showcasing the process for making World War chatbot, learning along the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734546a",
   "metadata": {},
   "source": [
    "Test Q/A:\n",
    "Questions:\n",
    "\n",
    "Who was the leader of Germany Army during World War II?\n",
    "\n",
    "When did the Battle of Midway take place?\n",
    "\n",
    "What country did Germany Invade in 1940?\n",
    "\n",
    "When did World War 2 Began?\n",
    "\n",
    "Which two countries signed the Molotov-Ribbentrop Pact?\n",
    "\n",
    "Who were the Axis Powers during World War II?\n",
    "\n",
    "When did World War 2 end?\n",
    "\n",
    "What event marked as start of World War II?\n",
    "\n",
    "What was D-day?\n",
    "\n",
    "When was D-day?\n",
    "\n",
    "Which two cities were hit with atomic bomb in 1945?\n",
    "\n",
    "What was the name of the German military strategy used in the early part of the war?\n",
    "\n",
    "What hate was hiroshima bombed?\n",
    "\n",
    "When did the Allies liberate Paris?\n",
    "\n",
    "When did Adolf Hitler die?\n",
    "\n",
    "When did Germany surrender in World War II?\n",
    "\n",
    "When did Japan surrender in World War II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0aeb9",
   "metadata": {},
   "source": [
    "Model 1: DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f443796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comibing .txt files together and preprocessing the text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "sources = {}\n",
    "for folder in [\"ww2_sources\", \"ww2_timelines\"]:\n",
    "    for file in os.listdir(folder):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = preprocess_text(f.read())\n",
    "            if len(content) > 100:\n",
    "                sources[file.replace(\".txt\", \"\")] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting q/a pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaking text into smaller chucks\n",
    "def chunk_text(text, max_words=300):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777605ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function searches for the best answer to the provided question across multiple text sources.\n",
    "def ask_question(question):\n",
    "    best = {\"score\": 0, \"answer\": \"Not found\", \"source\": None}\n",
    "    for file, content in sources.items():\n",
    "        for chunk in chunk_text(content):\n",
    "            try:\n",
    "                result = qa_pipeline(question=question, context=chunk)\n",
    "                if result[\"score\"] > best[\"score\"]:\n",
    "                    best.update({\"score\": result[\"score\"], \"answer\": result[\"answer\"], \"source\": file})\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    #Prining Confidence Score, Answer as well as source\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {best['answer']}\")\n",
    "    print(f\"Source: {best['source']}.txt\")\n",
    "    print(f\"Confidence: {best['score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuously prompts the user to ask WW2-related questions until pressed 'exit'.\n",
    "while True:\n",
    "    q = input(\"Ask a WW2 question (or type 'exit'): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "    ask_question(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6d0ef",
   "metadata": {},
   "source": [
    "Model 2: Roberta & Sentence Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all content together\n",
    "sources = {}\n",
    "for folder in [\"ww2_sources\", \"ww2_timelines\"]:\n",
    "    for file in os.listdir(folder):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = preprocess_text(f.read())\n",
    "            if len(content) > 100:\n",
    "                sources[file.replace(\".txt\", \"\")] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaking text into chunk\n",
    "def chunk_text(text, max_words=300):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks, current_chunk = [], []\n",
    "    word_count = 0\n",
    "    for sentence in sentences:\n",
    "        wc = len(sentence.split())\n",
    "        if word_count + wc > max_words:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk, word_count = [], 0\n",
    "        current_chunk.append(sentence)\n",
    "        word_count += wc\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building chunk index and vectorize with Sentence-BERT\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_index = []  # Each item: {embedding, text, source}\n",
    "\n",
    "for filename, content in sources.items():\n",
    "    for chunk in chunk_text(content):\n",
    "        if len(chunk.split()) >= 30:\n",
    "            embedding = model_embed.encode(chunk, convert_to_tensor=True)\n",
    "            chunk_index.append({\"embedding\": embedding, \"text\": chunk, \"source\": filename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading QA model- Roberta trained on SQuAD2\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q/a smart search\n",
    "def ask_question(question, top_k=3):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    q_vec = model_embed.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    #Find top-k most similar chunks\n",
    "    sims = [util.pytorch_cos_sim(q_vec, entry['embedding'])[0][0].item() for entry in chunk_index]\n",
    "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:top_k]\n",
    "\n",
    "    best = {\"score\": 0, \"answer\": \"Not found\", \"source\": None}\n",
    "    for i in top_indices:\n",
    "        entry = chunk_index[i]\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=entry[\"text\"])\n",
    "            if result['score'] > best['score'] and result['answer'].lower() != '':\n",
    "                best.update({\"score\": result['score'], \"answer\": result[\"answer\"], \"source\": entry['source']})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"Answer: {best['answer']}\")\n",
    "    print(f\"Source: {best['source']}.txt\")\n",
    "    print(f\"Confidence: {best['score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuously prompts the user to ask WW2-related questions until pressed 'exit'.\n",
    "while True:\n",
    "    user_input = input(\"Ask a WW2 question (or type 'exit'): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    ask_question(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe59d2",
   "metadata": {},
   "source": [
    "Model 3: Tuning chunks and confidence to check difference With Roberta and Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55353031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2edb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all content from sources\n",
    "sources = {}\n",
    "for folder in [\"ww2_sources\", \"ww2_timelines\"]:\n",
    "    for file in os.listdir(folder):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = preprocess_text(f.read())\n",
    "            if len(content) > 100:\n",
    "                sources[file.replace(\".txt\", \"\")] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13127cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smart chunking\n",
    "def chunk_text(text, max_words=300):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks, current_chunk = [], []\n",
    "    word_count = 0\n",
    "    for sentence in sentences:\n",
    "        if any(x in sentence.lower() for x in [\"sign up\", \"click\", \"cookies\", \"learn more\"]):\n",
    "            continue\n",
    "\n",
    "\n",
    "        wc = len(sentence.split())\n",
    "        if word_count + wc > max_words:\n",
    "            if word_count >= 30:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk, word_count = [], 0\n",
    "        current_chunk.append(sentence)\n",
    "        word_count += wc\n",
    "    if len(current_chunk) >= 1 and word_count >= 30:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vector index\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_index = []\n",
    "for filename, content in sources.items():\n",
    "    for chunk in chunk_text(content):\n",
    "        embedding = model_embed.encode(chunk, convert_to_tensor=True)\n",
    "        chunk_index.append({\"embedding\": embedding, \"text\": chunk, \"source\": filename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds and ranks top answers to a question using similarity scores and a confidence threshold.\n",
    "def ask_question(question, top_k=5, confidence_threshold=0.4):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    q_vec = model_embed.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    #Computing similarity scores\n",
    "    sims = [util.pytorch_cos_sim(q_vec, entry['embedding'])[0][0].item() for entry in chunk_index]\n",
    "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        entry = chunk_index[i]\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=entry[\"text\"])\n",
    "            if result['answer'].strip():\n",
    "                results.append({\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"score\": result[\"score\"],\n",
    "                    \"source\": entry[\"source\"],\n",
    "                    \"context\": entry[\"text\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on chunk from {entry['source']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not results:\n",
    "        print(\"No valid answers found.\")\n",
    "        return\n",
    "\n",
    "    #Sorting results by confidence * answer length\n",
    "    for r in results:\n",
    "        r[\"rank_score\"] = r[\"score\"] * len(r[\"answer\"].split())\n",
    "\n",
    "    best = max(results, key=lambda x: x[\"rank_score\"])\n",
    "\n",
    "    if best['score'] < confidence_threshold:\n",
    "        print(\"I'm not confident enough to answer that.\")\n",
    "    else:\n",
    "        print(f\"\\nTop Answer: {best['answer']}\")\n",
    "        print(f\"Source: {best['source']}.txt\")\n",
    "        print(f\"Confidence: {best['score']:.2f}\")\n",
    "        print(f\"Retrieved From:\\n{best['context'][:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continuously prompts the user to ask WW2-related questions until typed 'exit'.\n",
    "while True:\n",
    "    user_input = input(\"Ask a WW2 question (or type 'exit'): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    ask_question(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16471a6",
   "metadata": {},
   "source": [
    "Final Model: With Best output using Roberta and Sentence Transformer and vigorous cleaning and preprocessing for optimal answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ca7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text by applying the following transformations:\n",
    "    - Removes references in square brackets\n",
    "    - Collapses multiple spaces into a single space.\n",
    "    - Inserts spaces between lowercase and uppercase letters\n",
    "    - Ensures proper spacing after punctuation marks\n",
    "    - Removes non-ASCII characters to ensure compatibility.\n",
    "    - Trims leading and trailing whitespace.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "    text = re.sub(r'([,.;:!?])([^\\s])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads and preprocesses text files from specified folders, storing them\n",
    "sources = {}\n",
    "for folder in [\"ww2_sources\", \"ww2_timelines\"]:\n",
    "    for file in os.listdir(folder):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = preprocess_text(f.read())\n",
    "            if len(content) > 100:\n",
    "                sources[file.replace(\".txt\", \"\")] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=300):\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks, ensuring each chunk has a manageable number of words.\n",
    "\n",
    "    The function operates as follows:\n",
    "    - Splits the text into sentences based on punctuation marks\n",
    "    - Filters out sentences containing specific irrelevant phrases\n",
    "    - Groups sentences into chunks with a maximum word count of `max_words`.\n",
    "    - Ensures that chunks have at least 30 words to avoid overly short sections.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        max_words (int): Maximum number of words allowed per chunk. Default is 300.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks, where each chunk is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks, current_chunk = [], []\n",
    "    word_count = 0\n",
    "    for sentence in sentences:\n",
    "        if any(x in sentence.lower() for x in [\"sign up\", \"cookies\", \"learn more\"]):\n",
    "            continue\n",
    "        wc = len(sentence.split())\n",
    "        if word_count + wc > max_words:\n",
    "            if word_count >= 30:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk, word_count = [], 0\n",
    "        current_chunk.append(sentence)\n",
    "        word_count += wc\n",
    "    if len(current_chunk) >= 1 and word_count >= 30:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb30271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates embeddings for text chunks from sources using a sentence transformer model and stores them in 'chunk_index'.\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_index = []\n",
    "for filename, content in sources.items():\n",
    "    for chunk in chunk_text(content):\n",
    "        embedding = model_embed.encode(chunk, batch_size= 16,  show_progress_bar= True, convert_to_tensor=True)\n",
    "        chunk_index.append({\"embedding\": embedding, \"text\": chunk, \"source\": filename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, top_k=5, confidence_threshold=0.3):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes a user-provided question to retrieve the most relevant answers from a set of precomputed text embeddings.\n",
    "\n",
    "    The function works as follows:\n",
    "    1. Encodes the question using a sentence transformer model to generate its vector representation.\n",
    "    2. Computes similarity scores between the question vector and embeddings from the indexed text chunks.\n",
    "    3. Selects the top `top_k` chunks with the highest similarity scores for further processing.\n",
    "    4. Uses a QA pipeline to extract answers from the selected chunks and ranks them based on a confidence-weighted score.\n",
    "    5. Returns the best answer if its confidence score meets the specified threshold; otherwise, it notifies the user of insufficient confidence.\n",
    "\n",
    "    Args:\n",
    "        question: The user query to process.\n",
    "        top_k: Number of most similar chunks to retrieve.\n",
    "        confidence_threshold: Minimum confidence score required to present an answer.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the top answer, its source, confidence score, and the retrieved context directly.\n",
    "        If no valid answers are found or the confidence score is too low, an appropriate message is displayed.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    q_vec = model_embed.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    sims = [util.pytorch_cos_sim(q_vec, entry['embedding'])[0][0].item() for entry in chunk_index]\n",
    "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        entry = chunk_index[i]\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=entry[\"text\"])\n",
    "            if result['answer'].strip():\n",
    "                results.append({\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"score\": result[\"score\"],\n",
    "                    \"source\": entry[\"source\"],\n",
    "                    \"context\": entry[\"text\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on chunk from {entry['source']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not results:\n",
    "        print(\"No valid answers found.\")\n",
    "        return\n",
    "\n",
    "    for r in results:\n",
    "        r[\"rank_score\"] = r[\"score\"] * len(r[\"answer\"].split())\n",
    "\n",
    "    best = max(results, key=lambda x: x[\"rank_score\"])\n",
    "\n",
    "    if best['score'] < confidence_threshold:\n",
    "        print(\"I'm not confident enough to answer that :() \")\n",
    "    else:\n",
    "        print(f\"\\nTop Answer: {best['answer']}\")\n",
    "        print(f\"Source: {best['source']}.txt\")\n",
    "        print(f\"Confidence: {best['score']:.2f}\")\n",
    "        print(f\"\\nRetrieved Chunk:\\n{best['context']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb079b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuously prompts the user to ask WW2-related questions until typed 'exit'.\n",
    "while True:\n",
    "    user_input = input(\"Ask a WW2 question (or type 'exit'): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    ask_question(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4602370",
   "metadata": {},
   "source": [
    "UI with  Gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested questions\n",
    "suggested_questions = [\n",
    "    \"When did the Battle of Midway take place?\",\n",
    "    \"When did World War 2 begin?\",\n",
    "    \"What country did Germany invade in 1940?\"\n",
    "]\n",
    "\n",
    "#QA function\n",
    "def qa_interface(question):\n",
    "    q_vec = model_embed.encode(question, convert_to_tensor=True)\n",
    "    sims = [util.pytorch_cos_sim(q_vec, entry['embedding'])[0][0].item() for entry in chunk_index]\n",
    "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:5]\n",
    "\n",
    "    candidates = []\n",
    "    for i in top_indices:\n",
    "        entry = chunk_index[i]\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=entry[\"text\"])\n",
    "            if result['answer'].strip():\n",
    "                candidates.append({\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"score\": result[\"score\"],\n",
    "                    \"source\": entry[\"source\"],\n",
    "                    \"chunk\": entry[\"text\"]\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for c in candidates:\n",
    "        c[\"rank_score\"] = c[\"score\"] * len(c[\"answer\"].split())\n",
    "\n",
    "    best = max(candidates, key=lambda x: x[\"rank_score\"], default=None)\n",
    "\n",
    "    if not best or best[\"score\"] < 0.4:\n",
    "        return \"I'm not confident enough to answer that.\", \"\", \"\", \"\"\n",
    "\n",
    "    return best[\"answer\"], f\"{best['score']:.2f}\", best[\"source\"], best[\"chunk\"]\n",
    "\n",
    "#Defining theme\n",
    "custom_theme = gr.themes.Base(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"slate\",\n",
    "    neutral_hue=\"gray\"\n",
    ")\n",
    "\n",
    "#Interface\n",
    "with gr.Blocks(theme=custom_theme, title=\"World War 2 QA System\") as interface:\n",
    "    gr.Markdown(\"## Ask me about World War II\")\n",
    "    gr.Markdown(\"Use the input box or click a suggested question below:\")\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(lines=2, placeholder=\"Ask me a question...\", label=\"Your Question\")\n",
    "        ask_button = gr.Button(\"Answer me\")\n",
    "\n",
    "    # Add suggested question buttons\n",
    "    with gr.Row():\n",
    "        for q in suggested_questions:\n",
    "            gr.Button(q).click(fn=lambda q=q: q, outputs=question_input)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            answer_output = gr.Textbox(label=\"Answer\")\n",
    "            confidence_output = gr.Textbox(label=\"Confidence Score\")\n",
    "        with gr.Column():\n",
    "            source_output = gr.Textbox(label=\"Source File\")\n",
    "            chunk_output = gr.Textbox(label=\"Retrieved Chunk\", lines=8)\n",
    "\n",
    "    # Bind main button\n",
    "    ask_button.click(\n",
    "        fn=qa_interface,\n",
    "        inputs=question_input,\n",
    "        outputs=[answer_output, confidence_output, source_output, chunk_output]\n",
    "    )\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
